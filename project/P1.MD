All computations were performed in perlmutter

1. The overhead of parallelizing code refers to the computation associated with implementing parallelization, attributing to separating and communicating between threads. We see that running on one thread/serially has a compute time of 400-500 ms, but this time suddenly spikes to 1105 seconds when attempting to parallelize on 2 threads. This means the overhead is on the order of 600-700 ms. This is reasonable, as we increase the number of threads, parallel computing is never more efficient than 600-700 ms.

Because we see a time increase when we implement parallelization, we see that the overhead of implementing parallelism does not outweight the time saved.

2. As more threads are used, paralellization does reduce the time. At around its most effecient, parallelization executed to 1.43x serial computation. After which the time compute begins to increase. We can deduce that after a certain number of threads, the overhead associated with dividing and communicating between threads begins to outpace the time saved by parallelization.

3. Because we see a increase at a point where we increase the threads used, this algorithm is limited by the number of threads that can increase the overhead steps, meaning this algorithm is compute bound

